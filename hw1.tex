\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,fancyhdr,parskip,amssymb,graphicx}
\usepackage[all]{xy}
\usepackage[margin = 1.5in]{geometry}
\pagestyle{fancy}
\lhead{Ben Carriel (bkc39)}
\chead{Bryan Cuccioli (blc72)}
\rhead{Andy Levine (awl58)}
\setlength{\parindent}{1cm}

\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\R}{\mathbb{R}}

\DeclareMathOperator{\oh}{\mathcal{O}}
\DeclareMathOperator{\ta}{\xrightarrow{\ \ \ }}
\DeclareMathOperator{\opt}{\texttt{OPT}}

\newcommand{\problem}[1]{\noindent {\bf #1}}
\newcommand{\problempart}[1]{\noindent{\textbf{(#1)}}}

\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
\newtheorem*{claim}{Claim}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}

\begin{document}

\problem{Problem 1.} \\
The counterexample is the following graph:

\problem{Problem 2.} \\
Observe first that the following graph is the ``worst case'' in the sense that the Hopcroft-Karp algorithm creates the least efficient matching for it. We will proceed via an induction-like argument. Define a `z-shaped subgraph' to be one that looks like the following:

\begin{displaymath}\xygraph{
!{<0cm,0cm>;<1cm,0cm>:<0cm,1cm>::}
!{(0,0) }*+{\bullet_{a}}="a"
!{(1.5,0) }*+{\bullet_{b}}="b"
!{(0,1.5) }*+{\bullet_{c}}="c"
!{(1.5,1.5)}*+{\bullet_{d}}="d"
"a":"b"
"b":"c"
"c":"d"
}
\end{displaymath}

For $K_{2,2}$, then it is clear that the z-shaped subgraph is the worst case, because the other cases are just $K_{1,1}$ or two connected components made up of copies of $K_{1,1}$. Suppose now that for $K_{n-1,n-1}$, the worst case is a string of connected z-shaped subgraphs. Now considering $K_{n,n}$, adding two more vertices, we have the worst case matches them, because previously every node was matched with the one diagonally across from it. Hence the worst case matching is the one composed of a string of z-shaped subgraphs.

Then it is easy to see that $c_1=1/2$, because in the worst case the algorithm first chooses a path of length 1, and so in the worst case graph described above we have $|M_1|/|M^{\ast}|=1/2$.

On the second iteration, the algorithm picks blocking paths of size 2, and in the general $k$th step it picks blocking paths of size $k$. Hence the minimal matching is increasing by \emph{at least} 1 every iteration. We have $|M^{\ast}|$ as the least upper bound for $|M_k|$, so by a basic theorem of analysis, we must have $c_k\to 1$ as $k\to\infty$.

\problem{Problem 3} \\
The plan of attack for this algorithm is to reduce the half-integral matching problem to a bipartite matching problem and use the Hopkroft-Karp algorithm to find a maximum bipartite matching and then translate the resulting matching to a maximum size half integral matching in the original graph. As an input we are given a graph $\Gamma = (V,E)$. Each edge $e \in E$ has a weight $w(e) \in \{0, 1/2, 1\}$

\problem{Problem 4.} \\
\problempart{a} Fix a vertex $i \in L$. We want to compute the probability that $i$ will be chosen by the stateless rounding procedure. If we denote this event by 
\[
M = \{\text{vertex } i \text{ is chosen by the stateless rounding procedure}\}
\]
and let $N$ be its complement. Our goal is to compute $P(M)$. We will do this by computing $1 - P(N)$. We begin by noting that if we let 
\[
N_j = \{\text{vertex } i \text{ is chosen when vertex } j \text{ arrives}\}
\]
Then $N = \cap_{j \in R} N_j$. We can see that
\[
P(N_j) = 1 - x_{ij}z_j^{-1} \leq 1 - x_{ij}
\]
where the inequality holds because $z_j \leq 1$. This means that 
\[
P(N) = P(\bigcap_{j\in R} N_j) = \prod_{j\in R} P(N_j) \leq \prod_{j\in R} (1 - x_{ij}) 
\]
We will then use the following \\
\begin{lem}
For each $x \in \R$ we have that 
\[
1-x \leq e^{-x}
\]
\end{lem}
\begin{proof}
Consider the function $f(x) = e^{-x} + x - 1$. Then if $f(x) > 0$ for every $x$, we are done. If we take the derivative we see that $f'(x) = -e^{-x} + 1$, which has a critical point at $x = 0$. We then take the second derivative to reveal that $f''(x) = e^{-x} > 0$ for all $x$. This means that $f$ has a minimum at $f(0) = 0$, meaning that $f > 0$.  
\end{proof}
Using this lemma we can see that
\[
P(N) = \prod_{j\in R} (1 - x_{ij})  \leq \prod_{j\in R} e^{-x_{ij}}
\]
If we let $s = \sum_{j\in R}x_{ij}$ then $P(N) = e^{-s}$. In order to finally compute $P(M)$ we need one more \\
\begin{lem}
For $x \in [0, 1]$ we have 
\[
1 - e^{-x} \geq \frac{e-1}{e}x
\]
\end{lem}
\begin{proof}
This is similar to the previous lemma. This time we set $f(x) = 1 - e^{-x} - (1-1/e)x$ which has derivative $f'(x) = e^{-x} - 1 + e^{-1}$. The critical point occurs at $x_0 = 1 - \log(-1 + e)$, and if we look at the second derivative $f''(x) = -e^{-x}$ then we see that $x_0$ is a local maximum and that $f$ is non-negative on $[0,1]$. This gives the result.
\end{proof}
Finally, we can use this lemma to see that
\[
P(M) = 1 - P(N) \geq 1 - e^{-s} \geq (1 - 1/e)s 
\]
Substituting for $s$ gives the final result
\[
P(M) \geq 1 - 1/e)\sum_{j\in R} x_{ij}
\]
and we are done.

\problempart{b} 

\end{document}
